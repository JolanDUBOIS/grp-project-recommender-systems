{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "from src.main import data_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime: roughly 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalized data\n",
    "data, embeddings = data_normalization()\n",
    "\n",
    "# Extract relevant dataframes\n",
    "news_df = data[\"news\"]\n",
    "behaviors_df = data[\"behaviors\"]\n",
    "\n",
    "behaviors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mistake i did last time was to assume \"Clicked\" and \"News ID\" was already there - I assume that i planned to make them, but wrote the draft and forgot to actually do it. \n",
    "\n",
    "\"Impressions\" will now be split on \"-\" into \"News ID\" and \"Clicked\". I assume this is correct, feel free to correct me. Then \"User ID\" will be added through the old dataset.\n",
    "\n",
    "Runtime: 20 minutes - 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required columns exist\n",
    "if \"Impressions\" in behaviors_df.columns:\n",
    "    # Split \"Impressions\" to extract News ID and Clicked values\n",
    "    impressions_expanded = behaviors_df[\"Impressions\"].astype(str).str.split(\" \", expand=True).stack().reset_index(level=1, drop=True)\n",
    "    impressions_df = impressions_expanded.astype(str).str.split(\"-\", expand=True).rename(columns={0: \"News ID\", 1: \"Clicked\"})\n",
    "\n",
    "    # Clean the \"Clicked\" column by stripping unwanted characters\n",
    "    impressions_df[\"Clicked\"] = impressions_df[\"Clicked\"].str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "\n",
    "    # Convert \"clicked\" to type int\n",
    "    impressions_df[\"Clicked\"] = impressions_df[\"Clicked\"].astype(int)\n",
    "\n",
    "    # Removing the ' that appears in the beginning of every News ID\n",
    "    impressions_df[\"News ID\"] = impressions_df[\"News ID\"].str.strip(\"'\")  # Remove leading quotes\n",
    "\n",
    "    # Add back User ID from the original dataframe\n",
    "    impressions_df = impressions_df.join(behaviors_df[\"User ID\"], how=\"left\")\n",
    "    \n",
    "    print(impressions_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"Impressions is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps something Jolan can implement in the normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique Users: {impressions_df['User ID'].nunique()}\")\n",
    "print(f\"Unique News: {impressions_df['News ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of unique values, making the matrix incredibly large. I will make a sparse matrix instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating matrix and applying filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(impressions_df.head())\n",
    "print(impressions_df.dtypes)\n",
    "print(impressions_df[\"User ID\"].unique()[:10])  # Show first 10 unique User IDs\n",
    "print(impressions_df[\"News ID\"].unique()[:10])  # Show first 10 unique News IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert User ID and News ID to categorical codes (numeric indices)\n",
    "impressions_df[\"User Index\"] = impressions_df[\"User ID\"].astype(\"category\").cat.codes\n",
    "impressions_df[\"News Index\"] = impressions_df[\"News ID\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a User-Item interaction matrix will be made - first it will check for the necessary columns. A sparse matrix only stores entries where an item was interacted with, ignoring all those where the user didn't clikc on the article.\n",
    "\n",
    "Interaction matrices: these matrices are tables that represent user behaviour - what each user has interacted with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User-Item Interaction matrix\n",
    "if \"User ID\" in impressions_df.columns and \"News ID\" in impressions_df.columns:\n",
    "\n",
    "    # Ensure there are no duplicate User ID and News ID pair\n",
    "    impressions_df = impressions_df.groupby([\"User ID\", \"News ID\"])[\"Clicked\"].max().reset_index()\n",
    "    \n",
    "    # Create sparse interaction matrix\n",
    "    interaction_matrix = coo_matrix((impressions_df[\"Clicked\"], (impressions_df[\"User ID\"], impressions_df[\"News ID\"])), shape=(len(user_mapping), len(news_mapping)))\n",
    "\n",
    "    print(f\"Sparse Matrix Shape: {interaction_matrix.shape}\")\n",
    "    print(f\"Non-zero interactions: {interaction_matrix.nnz}\")  # Check sparsity\n",
    "\n",
    "else:\n",
    "    interaction_matrix = pd.DataFrame()\n",
    "    print(\"Missing columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is applied, given that the matrix isn't empty. This uses SVD.\n",
    "\n",
    "Collaborative filtering: finds similar users based on their history, and recommends similar items to similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply collaborative filtering\n",
    "if not interaction_matrix.empty:\n",
    "    U, sigma, Vt = svds(interaction_matrix, k=50) # Reduce dimensions\n",
    "    sigma = np.diag(sigma)\n",
    "    predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "    predicted_df = pd.DataFrame(predicted_ratings, index=interaction_matrix.index, columns=interaction_matrix.columns)\n",
    "    print(predicted_df.head())\n",
    "\n",
    "else:\n",
    "    predicted_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, content-based filtering is applied. Here we will be using TF-IDF (cosine similarity), but this may be changed in the future - perhaps into a word embedding method.\n",
    "\n",
    "Content-based filtering: recommends items by analyzing the attributes of items, and matching them with a user's preferences/past interactions. Measures similarity between items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply content-based filtering\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(news_df[\"Title\"] + \" \" + news_df[\"Abstract\"])\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Store results\n",
    "news_similarity_df = pd.DataFrame(similarity_matrix, index=news_df[\"News ID\"], columns=news_df[\"News ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid approach, combining scores\n",
    "def hybrid_recommendations(user_id, top_n=5, alpha=0.5):\n",
    "    if user_id not in predicted_df.index:\n",
    "        return [] # No recommendations for uknown/new users\n",
    "    \n",
    "    # Normalize collaborative scores\n",
    "    user_ratings = predicted_df.loc[user_id].copy()\n",
    "    user_ratings = (user_ratings - user_ratings.min()) / (user_ratings.max() - user_ratings.min()) # Normalize\n",
    "\n",
    "    # Compute content-based scores\n",
    "    content_scores = news_similarity_df[user_ratings.index].dot(user_ratings.fillna(0))\n",
    "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min()) # Normalize\n",
    "\n",
    "    # Combination of both scores, using weights\n",
    "    final_scores = alpha * user_ratings + (1 - alpha) * content_scores\n",
    "    return final_scores.nlargest(top_n).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the recommender\n",
    "if not interaction_matrix.empty:\n",
    "    user_id = interaction_matrix.index[0] # Pick a sample user\n",
    "    recommendations = hybrid_recommendations(user_id)\n",
    "    print(\"Recommended articles: \", recommendations)\n",
    "\n",
    "else:\n",
    "    print(\"No valid interactions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
