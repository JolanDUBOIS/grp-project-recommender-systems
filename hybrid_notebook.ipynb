{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "from src.main import data_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime: roughly 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Impression ID</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>History</th>\n",
       "      <th>Impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U134050</td>\n",
       "      <td>11/15/2019 8:55:22 AM</td>\n",
       "      <td>[N12246, N128820, N119226, N4065, N67770, N334...</td>\n",
       "      <td>[N91737-0, N30206-0, N54368-0, N117802-0, N181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U254959</td>\n",
       "      <td>11/15/2019 11:42:35 AM</td>\n",
       "      <td>[N34011, N9375, N67397, N7936, N118985, N10945...</td>\n",
       "      <td>[N119999-0, N24958-0, N104054-0, N33901-0, N92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U499841</td>\n",
       "      <td>11/15/2019 9:08:21 AM</td>\n",
       "      <td>[N63858, N26834, N6379, N85484, N15229, N65119...</td>\n",
       "      <td>[N18190-0, N89764-0, N91737-0, N54368-0, N4997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U107107</td>\n",
       "      <td>11/15/2019 5:50:31 AM</td>\n",
       "      <td>[N12959, N8085, N18389, N3758, N9740, N90543, ...</td>\n",
       "      <td>[N122944-1, N18190-0, N55801-0, N59297-0, N128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U492344</td>\n",
       "      <td>11/15/2019 5:02:25 AM</td>\n",
       "      <td>[N109183, N48453, N85005, N45706, N98923, N460...</td>\n",
       "      <td>[N64785-0, N82503-0, N32993-0, N122944-0, N291...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Impression ID  User ID                    Time  \\\n",
       "0              1  U134050   11/15/2019 8:55:22 AM   \n",
       "1              2  U254959  11/15/2019 11:42:35 AM   \n",
       "2              3  U499841   11/15/2019 9:08:21 AM   \n",
       "3              4  U107107   11/15/2019 5:50:31 AM   \n",
       "4              5  U492344   11/15/2019 5:02:25 AM   \n",
       "\n",
       "                                             History  \\\n",
       "0  [N12246, N128820, N119226, N4065, N67770, N334...   \n",
       "1  [N34011, N9375, N67397, N7936, N118985, N10945...   \n",
       "2  [N63858, N26834, N6379, N85484, N15229, N65119...   \n",
       "3  [N12959, N8085, N18389, N3758, N9740, N90543, ...   \n",
       "4  [N109183, N48453, N85005, N45706, N98923, N460...   \n",
       "\n",
       "                                         Impressions  \n",
       "0  [N91737-0, N30206-0, N54368-0, N117802-0, N181...  \n",
       "1  [N119999-0, N24958-0, N104054-0, N33901-0, N92...  \n",
       "2  [N18190-0, N89764-0, N91737-0, N54368-0, N4997...  \n",
       "3  [N122944-1, N18190-0, N55801-0, N59297-0, N128...  \n",
       "4  [N64785-0, N82503-0, N32993-0, N122944-0, N291...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the normalized data\n",
    "data, embeddings = data_normalization()\n",
    "\n",
    "# Extract relevant dataframes\n",
    "news_df = data[\"news\"]\n",
    "behaviors_df = data[\"behaviors\"]\n",
    "\n",
    "behaviors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mistake i did last time was to assume \"Clicked\" and \"News ID\" was already there - I assume that i planned to make them, but wrote the draft and forgot to actually do it. \n",
    "\n",
    "\"Impressions\" will now be split on \"-\" into \"News ID\" and \"Clicked\". Then \"User ID\" will be added through the old dataset.\n",
    "\n",
    "Runtime: 20 minutes - 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    News ID  Clicked  User ID\n",
      "0  ['N91737        0  U134050\n",
      "0    N30206        0  U134050\n",
      "0    N54368        0  U134050\n",
      "0   N117802        0  U134050\n",
      "0    N18190        0  U134050\n"
     ]
    }
   ],
   "source": [
    "# Ensure required columns exist\n",
    "if \"Impressions\" in behaviors_df.columns:\n",
    "    # Split \"Impressions\" to extract News ID and Clicked values\n",
    "    impressions_expanded = behaviors_df[\"Impressions\"].astype(str).str.split(\" \", expand=True).stack().reset_index(level=1, drop=True)\n",
    "    impressions_df = impressions_expanded.astype(str).str.split(\"-\", expand=True).rename(columns={0: \"News ID\", 1: \"Clicked\"})\n",
    "\n",
    "    # Clean the \"Clicked\" column by stripping unwanted characters\n",
    "    impressions_df[\"Clicked\"] = impressions_df[\"Clicked\"].str.replace(r\"[^\\d]\", \"\", regex=True).astype(int)\n",
    "\n",
    "    # Convert \"clicked\" to type int\n",
    "    impressions_df[\"Clicked\"] = impressions_df[\"Clicked\"].astype(int)\n",
    "\n",
    "    # Removing the ' that appears in the beginning of every News ID\n",
    "    impressions_df[\"News ID\"] = impressions_df[\"News ID\"].str.strip(\"'\")  # Remove leading quotes\n",
    "\n",
    "    # Add back User ID from the original dataframe\n",
    "    impressions_df = impressions_df.join(behaviors_df[\"User ID\"], how=\"left\")\n",
    "    \n",
    "    print(impressions_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"Impressions is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users: 255990\n",
      "Unique News: 9988\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Users: {impressions_df['User ID'].nunique()}\")\n",
    "print(f\"Unique News: {impressions_df['News ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of unique values, making the matrix incredibly large. I will make a sparse matrix instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating matrix and applying filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    News ID  Clicked  User ID\n",
      "0  ['N91737        0  U134050\n",
      "0    N30206        0  U134050\n",
      "0    N54368        0  U134050\n",
      "0   N117802        0  U134050\n",
      "0    N18190        0  U134050\n",
      "News ID    object\n",
      "Clicked     int32\n",
      "User ID    object\n",
      "dtype: object\n",
      "['U134050' 'U254959' 'U499841' 'U107107' 'U492344' 'U657892' 'U441763'\n",
      " 'U170615' 'U114779' 'U224919']\n",
      "[\"['N91737\" 'N30206' 'N54368' 'N117802' 'N18190' 'N122944' 'N69938'\n",
      " 'N18356' 'N123209' 'N46894']\n"
     ]
    }
   ],
   "source": [
    "print(impressions_df.head())\n",
    "print(impressions_df.dtypes)\n",
    "print(impressions_df[\"User ID\"].unique()[:10])  # Show first 10 unique User IDs\n",
    "print(impressions_df[\"News ID\"].unique()[:10])  # Show first 10 unique News IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert User ID and News ID to categorical codes (numeric indices)\n",
    "impressions_df[\"User Index\"] = impressions_df[\"User ID\"].astype(\"category\").cat.codes\n",
    "impressions_df[\"News Index\"] = impressions_df[\"News ID\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    News ID  Clicked  User ID  User Index  News Index\n",
      "0  ['N91737        0  U134050       11254        9785\n",
      "0    N30206        0  U134050       11254        2905\n",
      "0    N54368        0  U134050       11254        4371\n",
      "0   N117802        0  U134050       11254        1057\n",
      "0    N18190        0  U134050       11254        2122\n"
     ]
    }
   ],
   "source": [
    "print(impressions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a User-Item interaction matrix will be made - first it will check for the necessary columns. A sparse matrix only stores entries where an item was interacted with, ignoring all those where the user didn't clikc on the article.\n",
    "\n",
    "Interaction matrices: these matrices are tables that represent user behaviour - what each user has interacted with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix Shape: (255990, 9988)\n",
      "Non-zero interactions: 12722281\n"
     ]
    }
   ],
   "source": [
    "# Create User-Item Interaction matrix\n",
    "if \"User Index\" in impressions_df.columns and \"News Index\" in impressions_df.columns:\n",
    "\n",
    "    # Ensure there are no duplicate User ID and News ID pair\n",
    "    impressions_df = impressions_df.groupby([\"User Index\", \"News Index\"])[\"Clicked\"].max().reset_index()\n",
    "    \n",
    "    # Create sparse interaction matrix\n",
    "    interaction_matrix = coo_matrix((impressions_df[\"Clicked\"], (impressions_df[\"User Index\"], impressions_df[\"News Index\"])))\n",
    "\n",
    "    print(f\"Sparse Matrix Shape: {interaction_matrix.shape}\")\n",
    "    print(f\"Non-zero interactions: {interaction_matrix.nnz}\")  # Check sparsity\n",
    "\n",
    "else:\n",
    "    interaction_matrix = pd.DataFrame()\n",
    "    print(\"Missing columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is applied, given that the matrix isn't empty. This uses SVD.\n",
    "\n",
    "Collaborative filtering: finds similar users based on their history, and recommends similar items to similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.0 GiB for an array with shape (255990, 9988) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m U, sigma, Vt \u001b[38;5;241m=\u001b[39m svds(interaction_matrix\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m) \u001b[38;5;66;03m# Reduce dimensions\u001b[39;00m\n\u001b[0;32m      4\u001b[0m sigma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(sigma)\n\u001b[1;32m----> 5\u001b[0m predicted_ratings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m predicted_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predicted_ratings, index\u001b[38;5;241m=\u001b[39minteraction_matrix\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39minteraction_matrix\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 19.0 GiB for an array with shape (255990, 9988) and data type float64"
     ]
    }
   ],
   "source": [
    "# Apply collaborative filtering\n",
    "if interaction_matrix.nnz > 0:\n",
    "    U, sigma, Vt = svds(interaction_matrix.astype(float), k=50) # Reduce dimensions\n",
    "    sigma = np.diag(sigma)\n",
    "    predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "    predicted_df = pd.DataFrame(predicted_ratings, index=interaction_matrix.index, columns=interaction_matrix.columns)\n",
    "    print(predicted_df.head())\n",
    "\n",
    "else:\n",
    "    predicted_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, content-based filtering is applied. Here we will be using TF-IDF (cosine similarity), but this may be changed in the future - perhaps into a word embedding method.\n",
    "\n",
    "Content-based filtering: recommends items by analyzing the attributes of items, and matching them with a user's preferences/past interactions. Measures similarity between items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply content-based filtering\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(news_df[\"Title\"] + \" \" + news_df[\"Abstract\"])\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Store results\n",
    "news_similarity_df = pd.DataFrame(similarity_matrix, index=news_df[\"News ID\"], columns=news_df[\"News ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid approach, combining scores\n",
    "def hybrid_recommendations(user_id, top_n=5, alpha=0.5):\n",
    "    if user_id not in predicted_df.index:\n",
    "        return [] # No recommendations for uknown/new users\n",
    "    \n",
    "    # Normalize collaborative scores\n",
    "    user_ratings = predicted_df.loc[user_id].copy()\n",
    "    user_ratings = (user_ratings - user_ratings.min()) / (user_ratings.max() - user_ratings.min()) # Normalize\n",
    "\n",
    "    # Compute content-based scores\n",
    "    content_scores = news_similarity_df[user_ratings.index].dot(user_ratings.fillna(0))\n",
    "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min()) # Normalize\n",
    "\n",
    "    # Combination of both scores, using weights\n",
    "    final_scores = alpha * user_ratings + (1 - alpha) * content_scores\n",
    "    return final_scores.nlargest(top_n).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the recommender\n",
    "if not interaction_matrix.empty:\n",
    "    user_id = interaction_matrix.index[0] # Pick a sample user\n",
    "    recommendations = hybrid_recommendations(user_id)\n",
    "    print(\"Recommended articles: \", recommendations)\n",
    "\n",
    "else:\n",
    "    print(\"No valid interactions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
